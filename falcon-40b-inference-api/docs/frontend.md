# REST API client request return codes

The following return codes are implemented (see https://developer.mozilla.org/en-US/docs/Web/HTTP/Status):

* __200__: Success, returns HTTP 1.1 chunked encoding to send raw text of each generated token to client.
* __400__: Bad Request, returns `{"message": "Helpful debug message for client."}`, e.g. `Parameter: top_p is type=str, expected int`.
* __401__: Unauthorized
* __500__: Internal Server Error, this is default for unhandled exceptions, as defined in `proxy.py`
* __503__: Service Unavailable, returns `{"message": "Service overloaded, try again later."}`


# Frontend Overview: Exectution from inference_api_server.py to DecodeBackend.decode

1. The `inference_api_server.py` is a Flask server with routes for serving the Falcon 40B inference API. This WSGI server is run via gunicorn in production. The user prompt inference (generation) routes converge to `handle_inference()` that pushes user prompts to `input_queue` for decode backend processing. The input prompts are validated and optionally preprocessed, then the input queue size is checked for back pressure.
2. The Falcon 40B decode backend is started within `initialize_decode_backend` as a multiprocessing.Process running `run_decode_backend` imported from `decode_backend_v1.py`. Three `multiprocessing.Queue`s are used to communicate asynchronously with the backend:
    * input_queue: user prompts submitted
    * output_queue: generated tokens from backend
    * status_queue: backend status, e.g. number of users generation is running for
3. `decode_backend_v1.py` initializes `DecodeBackend` (see section below for detials) and runs an endless event loop in `run_generate` to:
    * `pick_prompts`: add prompts from `input_queue` to user rows for generation when capacity is available. Prompts are tokenized into token ids and prompt metadata is stored in `UserInfo` object.
    * `prepare_inputs`: pre-process input prompts tokens into:
        * input_ids:          1x32, predictions (long)
        * prompt_tokens:      32xseqlen padded prompts (long)
        * position_ids:       1x32 (long) tensor of position_ids for the currently generated token
        * prompt_lengths:     32, tensor of number of tokens in each prompt
        * attention_mask:     1x32x2048
        * kv_mask_id:         1x32
    * `decode`: run generation on all user rows in parallel, calls self.model.main_forward_part(...) details in DecodeBackend section below.
    * `switch_tokens`: manages next token selection for:
        * prefill: select next token in prompt_tokens
        * generation: select next generated token
        * cancelation: send EOS token
    * `push_outputs`: output generated tokens decoded to string to `output_queue`
    * `update_users`: remove user prompts from user rows upon EOS token.
    * `send_status`: put status information (input_queue.qsize(), num_users) in `status_queue`
4. The `output_queue` is read by `respond_to_users()` that runs on a single separate thread.
    * `output_queue`is read into (session_id, response)
    * `output_queue_map` is a map of queues keyed on session_id makes each response thread able to read the data for it's sepecific session_id.
    * `_reclaim_output_queues()` is run to remove resources for concluded sessions.
5. Individual responses are generated by `get_output` reading from their session_id's queue in `output_queue_map` and sent in `flask.Response` object to user client. The Response kwarg `content_type="text/event-stream"` uses HTTP 1.1 chunked euncoding allowing for each token to incrementally stream to the client.

